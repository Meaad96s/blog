---
layout: post
title: Project 3 | Predict Tree Cover
---

# Introduction

A classification problem to predict the cover type in the forest. The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. 

<p align="right">
  <img src="https://user-images.githubusercontent.com/20974667/66707339-8a48d280-ed47-11e9-9d7a-d7ff47451906.png" 
     width="300" height="300">
</p>

# Objective

Create a multi class model to predict Tree cover from Kaggle data.

# Dataset

Data Size : 239344
Source: Kaggle

## EDA
Check for:
1- Null values
2- Duplicate
3- Outliers

## Preprocessing

### Oversampling

There was class imbalance issue where  type 1 tree had over 500k and the other types only 2k.

<p align="right">
  <img src="https://user-images.githubusercontent.com/20974667/66707269-6a64df00-ed46-11e9-8e41-7ffdfd4726dd.png" 
     width="300" height="300">
</p>

Took a random sample of 50k of the whole data then SMOTE it.

<p align="right">
  <img src="https://user-images.githubusercontent.com/20974667/66764968-9487df80-eeb3-11e9-921a-d94212aecabc.png" 
     width="300" height="300">
</p>




### Standard Scaling

Bring all features varying in magnitudes, units and range to the same level of magnitudes


## Pipeline

Run multiple models with various combination of hyperparameter tuning.

1- Random Forest
2- Logistic Regression
3- stochiest Gradient Descent
4- Naive Bayes Classifier
5- Descision Tree

## Model selection and hyper tuning

Selected 4 models for final selection.
eleminated Naive Bayes Classifier since it underperforms the other models

## Summary and Analysis

### Results

Best results are shown in the table below:

| Models        |            | Best F1 score  |
| ------------- |:-------------:| -----:|
| Random Forest      | # of estimators=100,max depth=15 | 0.96 |
| Logistic Regression      | C:0.1, penalty: l1     |   0.45 |
| Decision Tree | max depth=3, minimum samples leaf=5   |    0.62 |

### Summary

* Random Forest has the best performance.
* Hyperparameter tuning does raise the performance to 5-7%


# Conclusion

* Random Forest does not need hyper tuning since it’s great
* If you need to oversample your data you might need to change the data itself




