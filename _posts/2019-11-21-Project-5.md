---
layout: post
title: Project 5 | Image Captioning
---

# Introduction

Image captioning is an importnt task that is needed for many applications. In simple words, analyzing images to generate automatic captions.
It is used for recommending captions in editing applications, usage in virtual assistants,  image indexing, and support of the disabled.



# Objective

Create a model to generate a caption from a picture.

# Dataset

Data Size : 8K
Source: Flicker


## Preprocessing
1. Resize
2. Normalize

### Image Feature Extraction

#### InceptionV3

42-layer deep learning network , 
factorizes Convolutions to reduce the number of parameters without decreasing the network efficiency.


## Image Model

1. InceptionV3
2. Dense layer


## Captions Processing

1. Consider only words which occur at least 10
2. Determine the maximum sequence length



## Global Vectors for Word Representation (Glove)

1. Word embedding lookup table
2. Word Embedding Matrix
3. LSTM RNN Model (experiment one)
4. GRU RNN Model (experiment two)


## Final Neural Network

1. Image Model
2. Language Model
3. Combined Model
4. Dense layer
5. Dense layer

### Results
<p align="center">
  <img src="https://user-images.githubusercontent.com/20974667/69316417-d8f06300-0c49-11ea-8f76-a30260d6f847.png" 
     width="300" height="300">
</p>

* Caption with LSTM: brown dog is running on the grass
* Caption with GRU: brown dog is digging hole with ball in its mouth

<p align="center">
  <img src="https://user-images.githubusercontent.com/20974667/69316426-dbeb5380-0c49-11ea-82fd-6cd1478ae830.png" 
     width="300" height="300">
</p>

* Caption with LSTM: two dogs are are are are in the grass
* Caption with GRU: white poodle is running through the grass


# Conclusion

* The System needs more improvement in terms of amount of data and resources.





